{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rawwong/DSI-Movie-Recommendation-System---NSDC-Data-Science-Projects/blob/main/DSI_Movie_Recommendation_System_NSDC_Data_Science_Projects.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1 align=\"center\">\n",
        "    NSDC Data Science Projects\n",
        "</h1>\n",
        "  \n",
        "<h2 align=\"center\">\n",
        "    June 2025 Data Camp Project: Movie Recommendation System\n",
        "</h2>\n",
        "<h3 align=\"center\">\n",
        "    Name: Rachel Wong\n",
        "</h3>\n"
      ],
      "metadata": {
        "id": "RTUk7S0GxzNv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Reminders:\n",
        "\n",
        "\n",
        "*   The DSI June 2025 Data Camp is restricted to Columbia DSI Students and will be held in an entirely virtual format starting on Friday, June 27th, 2025.\n",
        "\n",
        "*   The June 2025 Data Camp is an optional synchronized project meant to support incoming Columbia University DSI students. Students will be completing the project *individually,* but alongside peers and mentors!\n",
        "\n",
        "*   Look below for updates and important information that will help you build and complete your project.\n",
        "\n",
        "*   Visit the NSDC's dedicated [Slack Channel](https://join.slack.com/t/nsdcorps/shared_invite/zt-1gkd4ibdz-wGeQOwt3LUVooZDIK4zDPw) (#dsi-movie-recs-project) to stay in touch with the community.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "-BU_JrGEJBgx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Please read before you begin your project**\n",
        "\n",
        "**Instructions: Google Colab Notebooks:**\n",
        "\n",
        "Google Colab is a free cloud service. It is a hosted Jupyter notebook service that requires no setup to use, while providing free access to computing resources. We will be using Google Colab for this project.\n",
        "\n",
        "**In order to work within the Google Colab Notebook, please start by clicking on \"File\" and then \"Save a copy in Drive.\" This will save a copy of the notebook in your personal Google Drive.**\n",
        "\n",
        "Please rename the file to \"Movie Recommendation - Your Full Name - Your Columbia UNI.\" Once this project is completed, you will be prompted to share your file with the National Student Data Corps (NSDC) Project Leaders and Mentors.\n",
        "\n",
        "You can now start working on the project. :)"
      ],
      "metadata": {
        "id": "duXjuu5Ax1-z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll be using Google Colab for this assignment. This is a Python Notebook environment built by Google that's free for everyone and comes with a nice UI out of the box. For a comprehensive guide, see Colab's official guide [here](https://colab.research.google.com/github/prites18/NoteNote/blob/master/Welcome_To_Colaboratory.ipynb).\n",
        "\n",
        "Google Colab QuickStart Guide:\n",
        "- Notebooks are made up of cells, cells can be either text or code cells. Click the +code or +text button at the top to create a new cell\n",
        "- Text cells use a format called [Markdown](https://www.markdownguide.org/getting-started/). Cheatsheet is available [here](https://www.markdownguide.org/cheat-sheet/).\n",
        "- Python code is run/executed in code cells. You can click the play button at the top left of a code block (sometimes hidden in the square brackets) to run the code in that cell. You an also hit shift+enter to run the cell that is currently selected. There is no concurrency since cells run one at a time, but you can queue up multiple cells.\n",
        "- Each cell will run code individually but memory is shared across a notebook Runtime. You can think of a Runtime as a code session where everything you create and execute is temporarily stored. This means variables and functions are available between cells if you execute one cell before the other (physical ordering of cells does not matter). This also means that if you delete or change the name of something and re-execute the cell, the old data might still exist in the background. If things aren't making sense, you can always click Runtime -> restart runtime to start over.\n",
        "- Runtimes will persist for a short period of time so you are safe if you lose connection or refresh the page but Google will shutdown a runtime after enough time has past. Everything that was printed out will remain on the page even if the runtime is disconnected.\n",
        "- Google's Runtimes come preinstalled with all the core python libraries (math, rand, time, etc) as well as common data analysis libraries (numpy, pandas, scikitlearn, matplotlib). Simply run `import numpy as np` in a code cell to make it available."
      ],
      "metadata": {
        "id": "WHba4I24yBt8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Singular Value Decomposition**\n",
        "\n",
        "---\n",
        "\n",
        "# Crash Course on Singular Value Decomposition (SVD)\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Singular Value Decomposition, or SVD, is a mathematical technique used in many fields such as signal processing, statistics, and machine learning, particularly in the context of recommendation systems. It's a method for decomposing a matrix into three other matrices that reveal its underlying structure. Through this project, we will learn how to build a movie recommendation system using an SVD.\n",
        "\n",
        "## Basic Concepts\n",
        "\n",
        "### Matrices\n",
        "- **Matrix**: A rectangular array of numbers.\n",
        "- **Dimension of a Matrix**: Given in the form of rows × columns.\n",
        "\n",
        "### Decomposition\n",
        "- **Decomposition**: Breaking down a complex matrix into simpler, understandable parts.\n",
        "\n",
        "## What is SVD?\n",
        "\n",
        "```\n",
        "SVD breaks down any given matrix A into three separate matrices named U, Σ and V*\n",
        "ie. A = UΣV*\n",
        "```\n",
        "Where the components are:\n",
        "```\n",
        "- A: Original matrix.\n",
        "- U: Left singular vectors (orthogonal matrix).\n",
        "- Σ: Diagonal matrix of singular values (non-negative).\n",
        "- V*: Right singular vectors (conjugate transpose of V , an orthogonal matrix).\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "81bBnH8IyJIQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How do the predictions work?\n",
        "\n",
        "1. **Model Training**:\n",
        "   - The SVD algorithm is first trained on a portion of the dataset, which includes user ratings for various movies.\n",
        "   - During training, the model learns to associate certain patterns and characteristics of users and movies with specific rating behaviors.\n",
        "\n",
        "2. **Latent Features Extraction**:\n",
        "   - SVD decomposes the rating matrix into matrices representing latent features of users and movies.\n",
        "   - These latent features capture underlying aspects that affect rating behavior but are not explicitly available in the data (like user preferences or movie characteristics).\n",
        "\n",
        "3. **Making Predictions**:\n",
        "   - Once the model is trained, it can predict ratings for user-movie pairs where the actual rating is unknown.\n",
        "   - The prediction is essentially a dot product of the latent features of the user and the movie. It represents the estimated preference of the user for that particular movie based on the learned patterns.\n",
        "\n",
        "4. **Example of a Prediction**:\n",
        "   - Suppose we want to predict how user `U` would rate movie `M`.\n",
        "   - The model uses the latent features it has learned for user `U` and movie `M` to compute a predicted rating.\n",
        "   - This prediction is a numerical value, typically on the same scale as the original ratings (e.g., 1 to 5).\n",
        "\n",
        "5. **Application**:\n",
        "   - These predictions are used to recommend movies to users.\n",
        "   - For example, the system can recommend movies that have the highest predicted ratings for a particular user.\n",
        "\n",
        "6. **Handling New Users or Movies (Cold Start Problem)**:\n",
        "   - One challenge is predicting ratings for new users or movies that have little to no rating history. This is known as the cold start problem.\n",
        "   - Solutions might involve using content-based approaches or hybrid models that don't rely solely on historical rating data."
      ],
      "metadata": {
        "id": "ukvxW2AgTjdt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Where do we use SVDs?\n",
        "\n",
        "### Applications in Recommendation Systems\n",
        "\n",
        "In recommendation systems, SVD is used to predict unknown preferences by decomposing a large matrix of user-item interactions into factors representing latent features. It helps in capturing the underlying patterns in the data.\n",
        "\n",
        "### Process\n",
        "\n",
        "1. **Matrix Creation**: Start with a matrix where rows represent users, columns represent items, and entries represent user ratings.\n",
        "2. **Apply SVD**: Decompose this matrix using SVD.\n",
        "3. **Latent Features**: The decomposition reveals latent features that explain observed ratings.\n",
        "4. **Prediction**: Use the decomposed matrices to predict missing ratings.\n",
        "\n",
        "### Advantages of an SVD\n",
        "- Effective at uncovering latent features in the data.\n",
        "- Reduces dimensionality, making computations more manageable.\n",
        "\n",
        "### Limitations of an SVD\n",
        "- Assumes linear relationships in data.\n",
        "- Sensitive to missing data and outliers."
      ],
      "metadata": {
        "id": "dLwYe8rLJY_-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##The Dataset\n",
        "\n",
        "\n",
        "#### Dataset being used : **Movielens 100k dataset**\n",
        "\n",
        "- This specific dataset, often referred to as \"ml-100k,\" contains 100,000 ratings from 943 users on 1,682 movies. The data was collected through the MovieLens website during the seven-month period from September 19th, 1997 to April 22nd, 1998.\n",
        "\n",
        "- **Data Structure**: The dataset includes user ratings that range from 1 to 5. Additionally, it provides demographic information about the users (age, gender, occupation, etc.) and details about the movies (titles, genres).\n",
        "\n",
        "- **Usage**: It's a standard dataset used for implementing and testing recommender systems. Its size is manageable, making it a popular choice for educational purposes and for initial experimentation with recommendation algorithms.\n",
        "\n",
        "- **Significance**: The diversity in the dataset, both in terms of users and movie genres, provides a rich ground for analyzing different recommendation strategies, testing algorithms like SVD, and understanding user preferences and behavioral patterns.\n",
        "\n",
        "This dataset is an excellent starting point for anyone looking to delve into the world of recommender systems and practice with real-world data.\n",
        "\n",
        "\n",
        "Now, we will write some code to understand and explore the dataset."
      ],
      "metadata": {
        "id": "C_2UqcW9JmGT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##The Surprise Library\n",
        "\n",
        "- In this project we will be using Python's [Surprise library](https://surpriselib.com/), which is specifically used for developing recommendation systems\n",
        "- The library includes built-in datasets (like the Movielens 100k), algorithms (like SVD), splitting functions, grid search functions among others for model training"
      ],
      "metadata": {
        "id": "BQST1BwR5_vq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "p1Wau2cYJ3tW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Milestone 1: Setting up your Environment\n",
        "\n",
        "**Step 1:** Install a compatible version of [NumPy](https://numpy.org/) below. Simply run the following code to set up the proper environment within your notebook.\n"
      ],
      "metadata": {
        "id": "OWW__zq67Sc1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#You may have to restart the session after running this cell.\n",
        "#Run this only once, you can comment out this part of the code after.\n",
        "!pip install \"numpy<2.0\""
      ],
      "metadata": {
        "id": "QaSHADc27M0-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e73a778-effc-4000-f528-0db3cfec2a17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy<2.0 in /usr/local/lib/python3.11/dist-packages (1.26.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Run this only once, you can comment out this part of the code after.\n",
        "!pip install surprise"
      ],
      "metadata": {
        "id": "GuT-QnvyKoWC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "496515de-6cc7-4110-9aba-109a61c6c8db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: surprise in /usr/local/lib/python3.11/dist-packages (0.1)\n",
            "Requirement already satisfied: scikit-surprise in /usr/local/lib/python3.11/dist-packages (from surprise) (1.1.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-surprise->surprise) (1.5.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-surprise->surprise) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-surprise->surprise) (1.15.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing necessary modules for this project\n",
        "import pandas as pd\n",
        "from surprise import Dataset\n",
        "from surprise.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "Ty8MyZxKKfIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Installing the dataset from pandas, run this only once, you can comment out this part of the code after.\n",
        "!pip install pandas scikit-surprise"
      ],
      "metadata": {
        "id": "mGZBl3OnKUU8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70000b52-dae9-4515-e34e-5adcf3777139"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-surprise in /usr/local/lib/python3.11/dist-packages (1.1.4)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-surprise) (1.5.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-surprise) (1.15.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "3CkdpNXSJ5H-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Milestone 2: Data Importing and Exploration\n",
        "\n",
        "**Step 1:** Upload your dataset. Fill in the blanks below to import the 'ml-100k' dataset. Import the columns: user, item, rating, and timestamp. Then, print the first 5 rows of your dataset using the `df.head` function."
      ],
      "metadata": {
        "id": "o_X-4JjLHCGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = Dataset.load_builtin('ml-100k')\n",
        "df = pd.DataFrame(data.raw_ratings, columns=[\"user\", \"item\", \"rating\", \"timestamp\"])\n",
        "\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "QNnS2A8HKcgn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a202c9d6-6ba1-4ad7-d9f2-eacd4bcc91d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset ml-100k could not be found. Do you want to download it? [Y/n] Y\n",
            "Trying to download dataset from https://files.grouplens.org/datasets/movielens/ml-100k.zip...\n",
            "Done! Dataset ml-100k has been saved to /root/.surprise_data/ml-100k\n",
            "  user item  rating  timestamp\n",
            "0  196  242     3.0  881250949\n",
            "1  186  302     3.0  891717742\n",
            "2   22  377     1.0  878887116\n",
            "3  244   51     2.0  880606923\n",
            "4  166  346     1.0  886397596\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see the following columns:\n",
        "\n",
        "* **User ID**: A unique identifier for the user who provided the rating.\n",
        "\n",
        "* **Item ID (Movie ID)**: A unique identifier for the movie that was rated.\n",
        "\n",
        "* **Rating:** The rating given to the movie by the user. In the MovieLens 100k dataset, these ratings are typically on a scale of 1 to 5.\n",
        "\n",
        "* **Timestamp:** The time at which the rating was provided. The timestamp is usually in Unix time format, which counts seconds since the Unix epoch (January 1, 1970).\n",
        "\n"
      ],
      "metadata": {
        "id": "eU2Tw5HgNLVR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2:** Let's explore the dataset! Obtain a summary of the dataset using the `info` function."
      ],
      "metadata": {
        "id": "ATBME07EneJ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Code below this line:\n",
        "df.info"
      ],
      "metadata": {
        "id": "kReOwhaCny9E",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "213d06f5-8549-49ef-ff6a-5c4713b0c2bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method DataFrame.info of       user  item  rating  timestamp\n",
              "0      196   242     3.0  881250949\n",
              "1      186   302     3.0  891717742\n",
              "2       22   377     1.0  878887116\n",
              "3      244    51     2.0  880606923\n",
              "4      166   346     1.0  886397596\n",
              "...    ...   ...     ...        ...\n",
              "99995  880   476     3.0  880175444\n",
              "99996  716   204     5.0  879795543\n",
              "99997  276  1090     1.0  874795795\n",
              "99998   13   225     2.0  882399156\n",
              "99999   12   203     3.0  879959583\n",
              "\n",
              "[100000 rows x 4 columns]>"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>pandas.core.frame.DataFrame.info</b><br/>def info(verbose: bool | None=None, buf: WriteBuffer[str] | None=None, max_cols: int | None=None, memory_usage: bool | str | None=None, show_counts: bool | None=None) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py</a>Print a concise summary of a DataFrame.\n",
              "\n",
              "This method prints information about a DataFrame including\n",
              "the index dtype and columns, non-null values and memory usage.\n",
              "\n",
              "Parameters\n",
              "----------\n",
              "verbose : bool, optional\n",
              "    Whether to print the full summary. By default, the setting in\n",
              "    ``pandas.options.display.max_info_columns`` is followed.\n",
              "buf : writable buffer, defaults to sys.stdout\n",
              "    Where to send the output. By default, the output is printed to\n",
              "    sys.stdout. Pass a writable buffer if you need to further process\n",
              "    the output.\n",
              "max_cols : int, optional\n",
              "    When to switch from the verbose to the truncated output. If the\n",
              "    DataFrame has more than `max_cols` columns, the truncated output\n",
              "    is used. By default, the setting in\n",
              "    ``pandas.options.display.max_info_columns`` is used.\n",
              "memory_usage : bool, str, optional\n",
              "    Specifies whether total memory usage of the DataFrame\n",
              "    elements (including the index) should be displayed. By default,\n",
              "    this follows the ``pandas.options.display.memory_usage`` setting.\n",
              "\n",
              "    True always show memory usage. False never shows memory usage.\n",
              "    A value of &#x27;deep&#x27; is equivalent to &quot;True with deep introspection&quot;.\n",
              "    Memory usage is shown in human-readable units (base-2\n",
              "    representation). Without deep introspection a memory estimation is\n",
              "    made based in column dtype and number of rows assuming values\n",
              "    consume the same memory amount for corresponding dtypes. With deep\n",
              "    memory introspection, a real memory usage calculation is performed\n",
              "    at the cost of computational resources. See the\n",
              "    :ref:`Frequently Asked Questions &lt;df-memory-usage&gt;` for more\n",
              "    details.\n",
              "show_counts : bool, optional\n",
              "    Whether to show the non-null counts. By default, this is shown\n",
              "    only if the DataFrame is smaller than\n",
              "    ``pandas.options.display.max_info_rows`` and\n",
              "    ``pandas.options.display.max_info_columns``. A value of True always\n",
              "    shows the counts, and False never shows the counts.\n",
              "\n",
              "Returns\n",
              "-------\n",
              "None\n",
              "    This method prints a summary of a DataFrame and returns None.\n",
              "\n",
              "See Also\n",
              "--------\n",
              "DataFrame.describe: Generate descriptive statistics of DataFrame\n",
              "    columns.\n",
              "DataFrame.memory_usage: Memory usage of DataFrame columns.\n",
              "\n",
              "Examples\n",
              "--------\n",
              "&gt;&gt;&gt; int_values = [1, 2, 3, 4, 5]\n",
              "&gt;&gt;&gt; text_values = [&#x27;alpha&#x27;, &#x27;beta&#x27;, &#x27;gamma&#x27;, &#x27;delta&#x27;, &#x27;epsilon&#x27;]\n",
              "&gt;&gt;&gt; float_values = [0.0, 0.25, 0.5, 0.75, 1.0]\n",
              "&gt;&gt;&gt; df = pd.DataFrame({&quot;int_col&quot;: int_values, &quot;text_col&quot;: text_values,\n",
              "...                   &quot;float_col&quot;: float_values})\n",
              "&gt;&gt;&gt; df\n",
              "    int_col text_col  float_col\n",
              "0        1    alpha       0.00\n",
              "1        2     beta       0.25\n",
              "2        3    gamma       0.50\n",
              "3        4    delta       0.75\n",
              "4        5  epsilon       1.00\n",
              "\n",
              "Prints information of all columns:\n",
              "\n",
              "&gt;&gt;&gt; df.info(verbose=True)\n",
              "&lt;class &#x27;pandas.core.frame.DataFrame&#x27;&gt;\n",
              "RangeIndex: 5 entries, 0 to 4\n",
              "Data columns (total 3 columns):\n",
              " #   Column     Non-Null Count  Dtype\n",
              "---  ------     --------------  -----\n",
              " 0   int_col    5 non-null      int64\n",
              " 1   text_col   5 non-null      object\n",
              " 2   float_col  5 non-null      float64\n",
              "dtypes: float64(1), int64(1), object(1)\n",
              "memory usage: 248.0+ bytes\n",
              "\n",
              "Prints a summary of columns count and its dtypes but not per column\n",
              "information:\n",
              "\n",
              "&gt;&gt;&gt; df.info(verbose=False)\n",
              "&lt;class &#x27;pandas.core.frame.DataFrame&#x27;&gt;\n",
              "RangeIndex: 5 entries, 0 to 4\n",
              "Columns: 3 entries, int_col to float_col\n",
              "dtypes: float64(1), int64(1), object(1)\n",
              "memory usage: 248.0+ bytes\n",
              "\n",
              "Pipe output of DataFrame.info to buffer instead of sys.stdout, get\n",
              "buffer content and writes to a text file:\n",
              "\n",
              "&gt;&gt;&gt; import io\n",
              "&gt;&gt;&gt; buffer = io.StringIO()\n",
              "&gt;&gt;&gt; df.info(buf=buffer)\n",
              "&gt;&gt;&gt; s = buffer.getvalue()\n",
              "&gt;&gt;&gt; with open(&quot;df_info.txt&quot;, &quot;w&quot;,\n",
              "...           encoding=&quot;utf-8&quot;) as f:  # doctest: +SKIP\n",
              "...     f.write(s)\n",
              "260\n",
              "\n",
              "The `memory_usage` parameter allows deep introspection mode, specially\n",
              "useful for big DataFrames and fine-tune memory optimization:\n",
              "\n",
              "&gt;&gt;&gt; random_strings_array = np.random.choice([&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;], 10 ** 6)\n",
              "&gt;&gt;&gt; df = pd.DataFrame({\n",
              "...     &#x27;column_1&#x27;: np.random.choice([&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;], 10 ** 6),\n",
              "...     &#x27;column_2&#x27;: np.random.choice([&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;], 10 ** 6),\n",
              "...     &#x27;column_3&#x27;: np.random.choice([&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;], 10 ** 6)\n",
              "... })\n",
              "&gt;&gt;&gt; df.info()\n",
              "&lt;class &#x27;pandas.core.frame.DataFrame&#x27;&gt;\n",
              "RangeIndex: 1000000 entries, 0 to 999999\n",
              "Data columns (total 3 columns):\n",
              " #   Column    Non-Null Count    Dtype\n",
              "---  ------    --------------    -----\n",
              " 0   column_1  1000000 non-null  object\n",
              " 1   column_2  1000000 non-null  object\n",
              " 2   column_3  1000000 non-null  object\n",
              "dtypes: object(3)\n",
              "memory usage: 22.9+ MB\n",
              "\n",
              "&gt;&gt;&gt; df.info(memory_usage=&#x27;deep&#x27;)\n",
              "&lt;class &#x27;pandas.core.frame.DataFrame&#x27;&gt;\n",
              "RangeIndex: 1000000 entries, 0 to 999999\n",
              "Data columns (total 3 columns):\n",
              " #   Column    Non-Null Count    Dtype\n",
              "---  ------    --------------    -----\n",
              " 0   column_1  1000000 non-null  object\n",
              " 1   column_2  1000000 non-null  object\n",
              " 2   column_3  1000000 non-null  object\n",
              "dtypes: object(3)\n",
              "memory usage: 165.9 MB</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 3646);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3:** Next, let's describe the statistics of this dataset using the `describe` function. [Click here if you need a refresher on descriptive statistics](https://youtube.com/playlist?list=PLNs9ZO9jGtUBQfxw7YAmtZJPRiEpnwaNc&feature=shared)!"
      ],
      "metadata": {
        "id": "I1YRWlDuI115"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Code below this line:\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "9_ntUWFALcZw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "bbfb8ef0-9170-47d4-cb39-0aa12f5ad7ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              rating\n",
              "count  100000.000000\n",
              "mean        3.529860\n",
              "std         1.125674\n",
              "min         1.000000\n",
              "25%         3.000000\n",
              "50%         4.000000\n",
              "75%         4.000000\n",
              "max         5.000000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7c8c78ed-d6b3-4eed-bfc1-24afab127d58\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>100000.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>3.529860</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1.125674</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>3.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>4.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>4.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>5.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7c8c78ed-d6b3-4eed-bfc1-24afab127d58')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7c8c78ed-d6b3-4eed-bfc1-24afab127d58 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7c8c78ed-d6b3-4eed-bfc1-24afab127d58');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-c6d069b4-f334-40ee-b233-23020b1ee021\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c6d069b4-f334-40ee-b233-23020b1ee021')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-c6d069b4-f334-40ee-b233-23020b1ee021 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"rating\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 35354.245317453555,\n        \"min\": 1.0,\n        \"max\": 100000.0,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          100000.0,\n          3.52986,\n          4.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Concept Check:** List any 2 findings you may notice from your data exploration.\n",
        "\n",
        "\n",
        "1.    Respond here!\n",
        "2.    Respond here!\n",
        "\n"
      ],
      "metadata": {
        "id": "4P_FGuKpJJe2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "0HRcXxj4J7Ql"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Milestone 3: Data Preprocessing"
      ],
      "metadata": {
        "id": "elfyNvl-H0cU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we will do some data preprocessing. This will include checking for missing values and converting timestamps to a readable format.\n",
        "\n"
      ],
      "metadata": {
        "id": "sJoxe-JwMCKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1:** Check the dataset for missing values. Learn more about [missing values here](https://www.geeksforgeeks.org/ml-handling-missing-values/)."
      ],
      "metadata": {
        "id": "oDPWIAUTJfzp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "id": "RZ_yvaO3LdgO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d381b650-ab09-4a2a-c732-39aac3be8513"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user         0\n",
            "item         0\n",
            "rating       0\n",
            "timestamp    0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Concept Check:** Are there missing values? How do you know?\n",
        "\n",
        ">*  The function will output the number of missing (NaN) values per column. Since the value is 0 for each column, then the dataset contains no missing values.\n",
        "\n"
      ],
      "metadata": {
        "id": "dgtjvngqMb0i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2:** Convert the timestamp to a readable format using the `pd.to_datetime` function. Use seconds (s) as your unit. Need a refresher? [Check out this resource](https://www.geeksforgeeks.org/python/python-pandas-to_datetime/)."
      ],
      "metadata": {
        "id": "nkvRTRf3KhSi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert timestamp to a readable format\n",
        "df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s')\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "-k2Jp5r3MgGZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2139f9ac-8de0-441d-96a1-23527fefe7db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-15-2949332350.py:2: FutureWarning: The behavior of 'to_datetime' with 'unit' when parsing strings is deprecated. In a future version, strings will be parsed as datetime strings, matching the behavior without a 'unit'. To retain the old behavior, explicitly cast ints or floats to numeric type before calling to_datetime.\n",
            "  df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  user item  rating           timestamp\n",
            "0  196  242     3.0 1997-12-04 15:55:49\n",
            "1  186  302     3.0 1998-04-04 19:22:22\n",
            "2   22  377     1.0 1997-11-07 07:18:36\n",
            "3  244   51     2.0 1997-11-27 05:02:03\n",
            "4  166  346     1.0 1998-02-02 05:33:16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "2quzmS_OjliY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Milestone 4: Inspecting Training and Test Sets"
      ],
      "metadata": {
        "id": "4rbX3UgqLIDX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In machine learning, a dataset is typically split into a **training set** and a **test set** to evaluate a model's performance. The training set is used to teach the model, while the test set is used to assess how well the model makes predictions on unseen data. To learn more, [explore this resource](https://www.w3schools.com/python/python_ml_train_test.asp).\n",
        "\n",
        "**Step 1:** Split the data into a training set and a test set."
      ],
      "metadata": {
        "id": "DnBXnR7fLw8d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into a training set and a test set\n",
        "trainset, testset = train_test_split(data, test_size=0.20)"
      ],
      "metadata": {
        "id": "TgiWI1VtMsia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2:** Display the number of users and items in the training set.\n"
      ],
      "metadata": {
        "id": "WDSzvZ7TMv1V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the number of users and items in the training set\n",
        "print(f\"Number of users: {trainset.n_users}\")\n",
        "print(f\"Number of items: {trainset.n_items}\")"
      ],
      "metadata": {
        "id": "LF7oNwlFMnZf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8a3d504-4034-40ee-d44c-1761c126a3d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of users: 943\n",
            "Number of items: 1652\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3:** Display the first few elements of the test set."
      ],
      "metadata": {
        "id": "2uNAOBarOgSM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first few elements of the test set\n",
        "print(testset[:5])"
      ],
      "metadata": {
        "id": "ak6xs2OSOlTq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55da25eb-f5ca-4b52-a445-51df894700f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('650', '290', 2.0), ('482', '311', 4.0), ('872', '1', 3.0), ('234', '989', 2.0), ('836', '260', 2.0)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ZZJ6f3QpJ9Lp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Milestone 5: Hyperparameter Tuning and Model Training\n",
        "Hyperparameter tuning is a critical step in optimizing the performance of an SVD model. The goal is to find the best combination of parameters that results in the most accurate predictions or lowest error rates.\n",
        "\n",
        "#### Hyperparameters we will be tuning in this project:\n",
        "\n",
        "1. **`n_factors`**:\n",
        "   - Represents the number of latent factors (or features) to extract from the dataset.\n",
        "   - The values can range between **`10 to 200`** to test the model's performance with a varying number of factors. A higher number of factors can capture more complex patterns but may lead to overfitting and increased computation time.\n",
        "\n",
        "2. **`n_epochs`**:\n",
        "   - Refers to the number of iterations over the entire dataset during training.\n",
        "   - The values can range between **`10 to 50`** providing a range to evaluate whether more iterations improve model performance or lead to overtraining.\n",
        "\n",
        "3. **`lr_all`** (Learning Rate):\n",
        "   - Determines the step size at each iteration while moving toward a minimum of the loss function.\n",
        "   - The values can range between **`0.001 to 0.01`** to test how fast the model learns. A smaller learning rate may lead to more precise convergence but requires more epochs.\n",
        "\n",
        "4. **`reg_all`** (Regularization Term):\n",
        "   - Helps prevent overfitting by penalizing larger model parameters.\n",
        "   - The values can range between **`0.01 to 0.1`** offering a range to assess the impact of regularization on model performance. Higher regularization can reduce overfitting but may lead to underfitting.\n",
        "\n",
        "</br>\n",
        "\n",
        "**Step 1:** Using the information above, try **defining your own parameter grid** with approximately 2-3 values for each parameter. Test out different values within the range to see how the model changes!"
      ],
      "metadata": {
        "id": "Xm4HL8PL71UF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here's an example of a grid of SVD hyperparameters for tuning\n",
        "param_grid_example = {\n",
        "    'n_factors': [50, 100, 150],\n",
        "    'n_epochs': [20, 30],\n",
        "    'lr_all': [0.005, 0.010],\n",
        "    'reg_all': [0.02, 0.1]\n",
        "}\n"
      ],
      "metadata": {
        "id": "Yt6AeZAzOmrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your own grid below of SVD hyperparameters for tuning. Use the above information for some helpful hints!\n",
        "\n",
        "param_grid = {\n",
        "    'n_factors': [90, 120, 151],\n",
        "    'n_epochs': [27, 30],\n",
        "    'lr_all': [0.006, 0.0235],\n",
        "    'reg_all': [0.05, 0.094]\n",
        "}"
      ],
      "metadata": {
        "id": "1EX5c1UV9lgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2:** Now, practice using Random Search for hyperparameter tuning.\n"
      ],
      "metadata": {
        "id": "3HPuxuyOYZqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the distribution of SVD hyperparameters for tuning using Random Search. Feel free to tweak the intervals!\n",
        "from scipy.stats import randint, uniform\n",
        "\n",
        "param_dist = {\n",
        "    'n_factors': randint(20, 201), #randint picks a random integer within the interval\n",
        "    'n_epochs': randint(10, 51),\n",
        "    'lr_all': uniform(0.001, 0.05), #uniform picks a random number within the interval\n",
        "    'reg_all': uniform(0.0001, 0.1),\n",
        "}"
      ],
      "metadata": {
        "id": "ja4_vphTAv5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3:** Train the model with the following parameters. Review the concepts below to gain a deeper understanding of SVD.\n",
        "\n",
        "1. **`SVD`**:\n",
        "   - SVD is used to break down the user-item rating matrix into two smaller matrices - one that represents users and their preferences, and the other that represents items (in our case, movies!) and their characteristics.\n",
        "   - Under the hood, the algorithm predicts a rating using the following equation: </br></br>\n",
        "$$\n",
        "\\hat r_{u i} \\;=\\; \\mu \\;+\\; b_{u} \\;+\\; b_{i} \\;+\\; q_{i}^{T} p_{u}\n",
        "$$\n",
        "\n",
        "where:  \n",
        "- $\\mu$ is the average rating across all users,  \n",
        "- $b_{u}$ is the user bias,  \n",
        "- $b_{i}$ is the item (movie) bias,  \n",
        "- $p_{u}$ and $q_{i}$ are vectors that represent the user's and item's features.   </br></br>\n",
        "\n",
        "\n",
        "The algorithm learns these values by minimizing prediction error on the training data (with a regularization term to avoid overfitting).\n",
        "\n",
        "$$\n",
        "\\min_{b_u,\\,b_i,\\,p_u,\\,q_i}\\quad\n",
        "\\sum_{(u,i)\\in R_{\\text{train}}} \\Bigl(r_{u i} \\;-\\; \\hat r_{u i}\\Bigr)^{2}\n",
        "\\;+\\; \\lambda\\,\\Bigl(b_{u}^{2} \\;+\\; b_{i}^{2} \\;+\\; \\lVert p_{u}\\rVert^{2} \\;+\\; \\lVert q_{i}\\rVert^{2}\\Bigr)\n",
        "$$\n",
        "\n",
        "Here, $R_{\\text{train}}$ is the set of observed $(u,i)$ pairs in the training data, and $\\lambda$ is the regularization hyperparameter.\n",
        "\n",
        "\n",
        "> **`High Level Idea`**\n",
        ">\n",
        "> *   Start with a matrix R , the observed data (which might or might not have missing values)\n",
        "> *   Split the observed matrix into two component matrices (P and Q, these do not have missing values) by optimizing the above loss function\n",
        "> *   Reconstruct the entries (observed and missing) of R.\n",
        "\n",
        "\n",
        "2. **`param_grid`**:\n",
        "   - It defines the grid of parameters that will be tested. You'll test different combinations of hyperparameters to see which ones work best.\n",
        "   - Example: If `param_grid` is `{'n_factors': [50, 100], 'lr_all': [0.005, 0.01]}`, GridSearchCV will evaluate the SVD algorithm for all combinations of `n_factors` and `lr_all` from these lists.\n",
        "\n",
        "3. **`measures=['RMSE', 'MAE']`**:\n",
        "   - These are the performance metrics used to evaluate the algorithm.\n",
        "   - `RMSE` stands for [Root Mean Square Error](https://www.sciencedirect.com/topics/engineering/root-mean-square-error), and `MAE` stands for [Mean Absolute Error](https://www.sciencedirect.com/topics/engineering/mean-absolute-error). Both are common metrics for evaluating the accuracy of prediction algorithms, with lower values indicating better performance.\n",
        "\n",
        "4. **`cv=3`**:\n",
        "   - This specifies the number of folds for [cross-validation](https://www.geeksforgeeks.org/machine-learning/cross-validation-machine-learning/).\n",
        "   - In this context, `cv=3` means that a 3-fold cross-validation will be used. The dataset will be split into three parts: in each iteration, two parts will be used for training, and one part will be used for testing. This process repeats three times, each time with a different part used for testing."
      ],
      "metadata": {
        "id": "XbPGTyysRG5W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4:** Perform Grid Search with cross-validation to find the best hyperparameters for our model. The following cells may take some time to run."
      ],
      "metadata": {
        "id": "mwCw9WUiY_Du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from surprise.model_selection import cross_validate, train_test_split, GridSearchCV\n",
        "from surprise import SVD, Dataset, Reader, accuracy\n",
        "\n",
        "\n",
        "# First, we create a GridSearchCV object with the model, parameter grid, metrics and number of folds as the arguments. Hint: review the content above if you're stuck!\n",
        "gs = GridSearchCV(SVD, param_grid, measures=['RMSE', 'MAE'], cv=3)\n",
        "\n",
        "\n",
        "#The fit method will train the model for every combination of hyperparameters using 3-fold cross validation.\n",
        "gs.fit(data)"
      ],
      "metadata": {
        "id": "Ys3MuGk5Qph4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5:** Perform Random Search with cross-validation to find the best hyperparameter for our model."
      ],
      "metadata": {
        "id": "uoNxv-GRZcH_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from surprise.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Create RandomSearchCV object with same parameters. Two additional arguments include n_iter (number of hyperparameter combinations) and random_state (to ensure deterministic combinations). Set the random_state to 42.\n",
        "rs = RandomizedSearchCV(SVD, param_dist, measures=['RMSE', 'MAE'], cv=3, n_iter=20, random_state=42)\n",
        "\n",
        "\n",
        "#The fit method will train the model for every combination of hyperparameters using 3-fold cross validation.\n",
        "rs.fit(data)"
      ],
      "metadata": {
        "id": "KwUhtHRIA_gs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 6:** Print the best RMSE score and parameters."
      ],
      "metadata": {
        "id": "VfTBYu7ZZ4QV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Best score and parameters\n",
        "\n",
        "print(f\"Best RMSE: {gs.best_score['rmse']}\")\n",
        "print(f\"Best parameters: {gs.best_params['rmse']}\")\n",
        "\n",
        "print(f\"Best RMSE using Random Search: {rs.best_score['rmse']}\")\n",
        "print(f\"Best parameters using Random Search: {rs.best_params['rmse']}\")"
      ],
      "metadata": {
        "id": "J85rTs0wQpYT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bf1452b-90a6-455c-b8d3-84128d26fb6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best RMSE: 0.922589019998891\n",
            "Best parameters: {'n_factors': 151, 'n_epochs': 27, 'lr_all': 0.0235, 'reg_all': 0.094}\n",
            "Best RMSE using Random Search: 0.9234425894780106\n",
            "Best parameters using Random Search: {'lr_all': 0.023524962598477153, 'n_epochs': 27, 'n_factors': 151, 'reg_all': 0.09432017556848528}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 7:** Now that you've tested different parameter combinations, use the best-performing model from your grid search. You can access it using the best_estimator attribute from your GridSearchCV object (gs) or the RandomSearchCV object (rs)."
      ],
      "metadata": {
        "id": "gXdWY3fGmVQp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Use the best model. Use the `best_estimator` function on gs/rs\n",
        "algo = gs.best_estimator['rmse']"
      ],
      "metadata": {
        "id": "JVCuCpr0RRMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train and test split. Make sure test_size is 0.25\n",
        "trainset, testset = train_test_split(data, test_size=0.25)\n",
        "\n",
        "#Fit the trainset to train the model\n",
        "algo.fit(trainset)\n",
        "\n",
        "#Make predictions on the testset\n",
        "predictions = algo.test(testset)\n",
        "\n",
        "#Calculate and print RMSE on the predictions made\n",
        "print(accuracy.rmse(predictions))"
      ],
      "metadata": {
        "id": "N_rMgmt3SyGb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec2c0046-a371-496d-9fea-71b5a66ad26b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE: 0.9112\n",
            "0.9112179182092204\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 8:** To understand the concept of cross-validation further, try changing `cv=3` to `cv=5`. To do this, define a second GridSearchCV and RandomSearchCV model (gs2 and rs2) and fit it to the data. Then find the best RMSE and parameters.  Finally, respond to the **concept check** below. These cells may take some time to run!\n",
        "\n"
      ],
      "metadata": {
        "id": "EYFWuQFl9_tv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gs2 = GridSearchCV(SVD, param_grid, measures=['RMSE', 'MAE'], cv=5)\n",
        "gs2.fit(data)\n",
        "\n",
        "rs2 = RandomizedSearchCV(SVD, param_dist, measures=['RMSE', 'MAE'], cv=5, n_iter=20, random_state=42) #Hint: use the code in Steps 4 & 5 to help if you're stuck! Simply fill in the blanks as we did previously. Remember to change cv to 5!\n",
        "rs2.fit(data)"
      ],
      "metadata": {
        "id": "MGvOk6FR-nrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Best RMSE: {gs2.best_score['rmse']}\")\n",
        "print(f\"Best parameters: {gs2.best_params['rmse']}\")\n",
        "\n",
        "print(f\"Best RMSE: {rs2.best_score['rmse']}\")\n",
        "print(f\"Best parameters: {rs2.best_params['rmse']}\") #Hint: use the code in Step 6 to help if you're stuck."
      ],
      "metadata": {
        "id": "XzIFr6KfAOUk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9edd4eb7-6819-41c6-d223-ca15946cea52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best RMSE: 0.913225665909964\n",
            "Best parameters: {'n_factors': 151, 'n_epochs': 27, 'lr_all': 0.0235, 'reg_all': 0.094}\n",
            "Best RMSE: 0.9133596445782564\n",
            "Best parameters: {'lr_all': 0.023524962598477153, 'n_epochs': 27, 'n_factors': 151, 'reg_all': 0.09432017556848528}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the best estimator, split the dataset and fit the training set to the model. Then, make predictions on the test set.\n",
        "algo2 = rs.best_estimator['rmse']"
      ],
      "metadata": {
        "id": "w8DjCGjrAX5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainset, testset = train_test_split(data, test_size=0.25) # idk\n",
        "algo2.fit(trainset)\n",
        "predictions2 = algo2.test(testset)\n",
        "accuracy.rmse(predictions)"
      ],
      "metadata": {
        "id": "D4B-T59yAbbU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e43574c-70b1-4d05-cba3-644fc0e056cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE: 0.9158\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9157836552935479"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Concept Check:** Report back on how the RMSE changes when changing `cv=3` to `cv=5`. Does it increase/decrease/stay the same?\n",
        "\n",
        ">*  By increasing the number of cross-validation folds from 3 to 5, the best RMSE score decreased. This means that the RMSE improved when switching from 3-fold to 5-fold cross-validation, suggesting that the model generalizes better with more varied validation splits."
      ],
      "metadata": {
        "id": "RsCUO9VhcEeD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 9:** Now we will use the first model to predict new user and item values."
      ],
      "metadata": {
        "id": "EGvo03v1BCqB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Predict rating for a user and item\n",
        "user_id = '196'  # replace with a specific user ID\n",
        "item_id = '302'  # replace with a specific item (movie) ID\n",
        "predicted_rating = algo.predict(user_id, item_id)\n",
        "print(f\"Predicted rating for user {user_id} and item {item_id}: {predicted_rating.est}\")"
      ],
      "metadata": {
        "id": "Qn7yRR-aRXL-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bb37c3e-8e17-430d-9559-0e1d79f7aa0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted rating for user 196 and item 302: 3.895047049050553\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To inspect the predictions in detail, let's print the first 10 predictions made by the model\n",
        "for idx, prediction in enumerate(predictions[:10]):\n",
        "    print(f'Prediction {idx}: User {prediction.uid} and item {prediction.iid} has true rating {prediction.r_ui}, and the predicted rating is {prediction.est}')\n"
      ],
      "metadata": {
        "id": "TD1GVZ3FpxdA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5dcf5f6f-a1ef-4b0f-9628-3fdd86f2c54a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction 0: User 286 and item 1074 has true rating 4.0, and the predicted rating is 3.502103837845117\n",
            "Prediction 1: User 276 and item 452 has true rating 3.0, and the predicted rating is 2.8437389791921817\n",
            "Prediction 2: User 733 and item 125 has true rating 2.0, and the predicted rating is 2.914217992389738\n",
            "Prediction 3: User 13 and item 59 has true rating 4.0, and the predicted rating is 4.3191918877872775\n",
            "Prediction 4: User 474 and item 737 has true rating 4.0, and the predicted rating is 3.7387711568438435\n",
            "Prediction 5: User 246 and item 1044 has true rating 1.0, and the predicted rating is 2.838956047495981\n",
            "Prediction 6: User 903 and item 185 has true rating 5.0, and the predicted rating is 4.544981570680432\n",
            "Prediction 7: User 95 and item 779 has true rating 3.0, and the predicted rating is 2.3908631107637754\n",
            "Prediction 8: User 405 and item 1590 has true rating 1.0, and the predicted rating is 2.301079448226717\n",
            "Prediction 9: User 457 and item 425 has true rating 4.0, and the predicted rating is 4.152637372262835\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "kmdHweoqjiw8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Milestone 6: Rounding Numbers\n",
        "Rounding values is a technique used to simplify numbers, but its appropriateness depends on the context.\n",
        "\n",
        "**When to Round**\n",
        "1. **Simplification**: For estimations.\n",
        "2. **Reporting**: When exact figures aren't necessary (e.g., in everyday language).\n",
        "3. **Data Analysis**: To focus on significant trends by ignoring minor variations.\n",
        "4. **Financial Transactions**: Rounding to the smallest currency unit.\n",
        "5. **Display Purposes**: For clarity in graphs or tables.\n",
        "\n",
        "**When NOT to Round**\n",
        "1. **Intermediate Calculations**: Early rounding can lead to significant final errors.\n",
        "2. **Legal/Regulatory Documents**: Require exact figures.\n",
        "3. **Scientific/Engineering Work**: Precision is crucial.\n",
        "4. **Critical Calculations**: In health, safety, or finance, precision is essential.\n",
        "\n",
        "To summarize,\n",
        "- Rounding depends on the purpose and context of the calculation.\n",
        "- It is useful for simplification and clarity but should be avoided when precision is critical.\n",
        "- We must be aware of potential cumulative errors in sequential calculations.\n"
      ],
      "metadata": {
        "id": "idnQfHixB0HB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us round the values of the predictions so that it falls within the rating categories of [1.0, 2.0, 3.0, 4.0, 5.0]"
      ],
      "metadata": {
        "id": "8TW56IrZLYej"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1:** Import math library for math.ceil"
      ],
      "metadata": {
        "id": "xuilkwrTCBZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math"
      ],
      "metadata": {
        "id": "xAhST8gpB_on"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2:** Round the prediction.est variable."
      ],
      "metadata": {
        "id": "7Lgm6hDTfRGW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO - Round the prediction.est variable being printed. Use python's default rounding function to achieve this\n",
        "\n",
        "for idx, prediction in enumerate(predictions[:10]):\n",
        "    temp = math.ceil(int(prediction.est))\n",
        "    print(f'Prediction {idx}: User {prediction.uid} and item {prediction.iid} has true rating {prediction.r_ui}, and the predicted rating is {round(prediction.est)}')\n"
      ],
      "metadata": {
        "id": "qoKsEX-TLYEw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee0e19ed-8751-4387-9b00-317c236fbda2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction 0: User 286 and item 1074 has true rating 4.0, and the predicted rating is 4\n",
            "Prediction 1: User 276 and item 452 has true rating 3.0, and the predicted rating is 3\n",
            "Prediction 2: User 733 and item 125 has true rating 2.0, and the predicted rating is 3\n",
            "Prediction 3: User 13 and item 59 has true rating 4.0, and the predicted rating is 4\n",
            "Prediction 4: User 474 and item 737 has true rating 4.0, and the predicted rating is 4\n",
            "Prediction 5: User 246 and item 1044 has true rating 1.0, and the predicted rating is 3\n",
            "Prediction 6: User 903 and item 185 has true rating 5.0, and the predicted rating is 5\n",
            "Prediction 7: User 95 and item 779 has true rating 3.0, and the predicted rating is 2\n",
            "Prediction 8: User 405 and item 1590 has true rating 1.0, and the predicted rating is 2\n",
            "Prediction 9: User 457 and item 425 has true rating 4.0, and the predicted rating is 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "iIlS6Amxjfah"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UT8Rpdt4ZPeA"
      },
      "source": [
        "<h3 align = 'center' >\n",
        "Thank you for completing the project!\n",
        "</h3>\n",
        "\n",
        "###**[Please submit all materials to the NSDC HQ team via this Google Form](https://docs.google.com/forms/d/e/1FAIpQLSfHQ2TYokDwXcR2X9s3_syFdZVi_gK7bvgFvjalxwTFzkBoLQ/viewform). **\n",
        "\n",
        "You may submit your project at any time. The June 27th deadline is a recommendation only. The Project Team will send out certificates to participants no sooner than Thursday, July 3rd. Please be patient with our small team as we process everyone's submissions.\n",
        "\n",
        "Participants who submit completed projects will receive a virtual certificate of completion. Do reach out to us if you have any questions or concerns at nsdc@columbia.edu. We are here to help you learn and grow.\n"
      ]
    }
  ]
}